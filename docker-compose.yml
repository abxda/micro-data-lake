version: "3"
#Micro Data Lake Services by @abxda
services:
  # Notebooks
  jupyter:
    build: ./services/jupyter-spark/
    env_file:
      - ./config/jupyter.env
    restart: always
    container_name: jupyter
    volumes: 
      - ./notebooks:/home/jovyan/work
    ports:
      - "8181:8888"
    entrypoint: sh -c 'start-notebook.sh --NotebookApp.token=$$JUPYTER_PASSWORD'
  # Spark Master
  spark-master:
    build: ./services/jupyter-spark/
    restart: always
    env_file: 
      - ./config/spark.env
    entrypoint: ["/bin/bash", "/master.sh"]
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
  # Spark Worker
  spark-worker-1:
      build: ./services/jupyter-spark/
      restart: always
      entrypoint: ["/bin/bash", "/worker.sh"]
      container_name: spark-worker-1
      depends_on:
        - spark-master
      ports:
        - "8081:8081"
      environment:
        - "SPARK_MASTER=spark://spark-master:7077"
  # Hadoop SQL
  hive:
    build: ./services/spark-hive/
    restart: always
    entrypoint: /init-hive.sh
    container_name: hive
    ports:
      - "9083:9083"
      - "10000:10000"
    links:
      - "postgres:postgres"
    depends_on:
      - "postgres"
    env_file: 
      - ./config/postgres.env
  # Relational SQL
  postgres:
    image: postgres
    restart: always
    container_name: postgres
    env_file: 
      - ./config/postgres.env
      - ./config/superset_database.env
      - ./config/airflow_database.env
      - ./config/shared_database.env
      - ./config/hive_database.env
    volumes:
      - postgres_volume:/var/lib/postgresql/data/
      - ./services/postgresql/init/:/docker-entrypoint-initdb.d/  
    ports:
      - 5432:5432
    restart: "always"
  # Database explorer
  pgadmin:
    container_name: "pgadmin"
    image: dpage/pgadmin4
    env_file:
      - ./config/pgadmin.env
    volumes:
        - pgadmin_volume:/root/.pgadmin
    ports:
      - 5050:80
    restart: "always"
  # Spark SQL
  spark-sql:
    build: ./services/spark-hive/
    restart: always
    entrypoint: /init-spark.sh
    container_name: spark-sql
    depends_on:
      - spark-master
    ports:
      - "10001:10001"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    links:
      - "postgres:postgres"
      - "hive:hive"
  # Model Storage
  minio:
    container_name: "minio"
    restart: "always"
    image: minio/minio
    env_file:
        - ./config/minio.env
    volumes:
        - minio_volume:/data
    ports:
        - 9000:9000
    command: server /data
  # API
  fastapi:
    container_name: "fastapi"
    restart: "always"
    build: services/fastapi
    env_file:
      - ./config/minio.env
    volumes:
      - ./services/fastapi/app:/app
    ports:
      - 80:80
  # Visualization
  superset:
    container_name: "superset"
    restart: "always"
    image: tylerfowler/superset:0.24
    depends_on:
        - postgres
    env_file:
        - ./config/superset_container.env
        - ./config/superset_database.env
    ports:
      - 8088:8088
  # Scheduling
  airflow:
    container_name: "airflow"
    restart: "always"
    build: services/airflow
    depends_on:
      - postgres
    env_file:
      - ./config/airflow_container.env
      - ./config/minio.env
      - ./config/shared_database.env
    volumes:
      - /services/airflow/dags/:/usr/local/airflow/dags
    ports:
      - 7777:8080
    command: webserver
  # Volumnes
volumes:
  postgres_volume:
  pgadmin_volume:
  minio_volume: